This paper explores the use of GPU technology in the training of multiple linear regression models. Multiple linear regression analysis is a commonly used statistical method in many fields, including finance, marketing, and social sciences. However, it can be computationally intensive, especially
when working with large datasets. The use of GPUs in parallel computing has been shown to significantly speed up training time for machine learning models, and this project investigates the potential benefits of using GPUs in multiple linear regression analysis.
The paper presents experimental results comparing the training time and performance of multiple regression models on a CPU versus a GPU. Our results showed that the use of GPUs can obtain good accuracy for the predictions, nevertheless, this time we did not obtain better performance for the GPU implementation with CUDA. Still there are several more things that we could try in the
future to try to improve the CUDA implementation and reach better performance.
Overall, this paper highlights the potential benefits of using GPU technology in multiple linear regression analysis and provides a good evaluation of different implementations of linear regression on CPU and GPU that can be used as reference in the future.

The documentation for the project can be found in "Documentation Project.pdf".

You can reach out to me at bustamantesuareza3@gmail.com
